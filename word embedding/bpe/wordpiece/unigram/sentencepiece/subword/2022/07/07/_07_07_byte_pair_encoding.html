<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Byte-Pair Encoding (BPE) 알아보기 | WJL in NLP</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Byte-Pair Encoding (BPE) 알아보기" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="BPE, WordPiece, Unigram, SentencePiece" />
<meta property="og:description" content="BPE, WordPiece, Unigram, SentencePiece" />
<link rel="canonical" href="https://wjlee-ling.github.io/nlp_log/word%20embedding/bpe/wordpiece/unigram/sentencepiece/subword/2022/07/07/_07_07_byte_pair_encoding.html" />
<meta property="og:url" content="https://wjlee-ling.github.io/nlp_log/word%20embedding/bpe/wordpiece/unigram/sentencepiece/subword/2022/07/07/_07_07_byte_pair_encoding.html" />
<meta property="og:site_name" content="WJL in NLP" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-07-07T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Byte-Pair Encoding (BPE) 알아보기" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-07-07T00:00:00-05:00","datePublished":"2022-07-07T00:00:00-05:00","description":"BPE, WordPiece, Unigram, SentencePiece","headline":"Byte-Pair Encoding (BPE) 알아보기","mainEntityOfPage":{"@type":"WebPage","@id":"https://wjlee-ling.github.io/nlp_log/word%20embedding/bpe/wordpiece/unigram/sentencepiece/subword/2022/07/07/_07_07_byte_pair_encoding.html"},"url":"https://wjlee-ling.github.io/nlp_log/word%20embedding/bpe/wordpiece/unigram/sentencepiece/subword/2022/07/07/_07_07_byte_pair_encoding.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/nlp_log/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://wjlee-ling.github.io/nlp_log/feed.xml" title="WJL in NLP" /><link rel="shortcut icon" type="image/x-icon" href="/nlp_log/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/nlp_log/">WJL in NLP</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/nlp_log/about/">About Me</a><a class="page-link" href="/nlp_log/search/">Search</a><a class="page-link" href="/nlp_log/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Byte-Pair Encoding (BPE) 알아보기</h1><p class="page-description">BPE, WordPiece, Unigram, SentencePiece</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-07-07T00:00:00-05:00" itemprop="datePublished">
        Jul 7, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/nlp_log/categories/#word embedding">word embedding</a>
        &nbsp;
      
        <a class="category-tags-link" href="/nlp_log/categories/#BPE">BPE</a>
        &nbsp;
      
        <a class="category-tags-link" href="/nlp_log/categories/#wordpiece">wordpiece</a>
        &nbsp;
      
        <a class="category-tags-link" href="/nlp_log/categories/#unigram">unigram</a>
        &nbsp;
      
        <a class="category-tags-link" href="/nlp_log/categories/#sentencepiece">sentencepiece</a>
        &nbsp;
      
        <a class="category-tags-link" href="/nlp_log/categories/#subword">subword</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/wjlee-ling/nlp_log/tree/master/_notebooks/2022_07_07_byte_pair_encoding.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/nlp_log/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          
          <div class="px-2">
    <a href="https://colab.research.google.com/github/wjlee-ling/nlp_log/blob/master/_notebooks/2022_07_07_byte_pair_encoding.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/nlp_log/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
          
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2"><a href="#기본-아이디어">기본 아이디어 </a></li>
<li class="toc-entry toc-h2"><a href="#Byte-level-BPE">Byte-level BPE </a></li>
<li class="toc-entry toc-h2"><a href="#WordPiece">WordPiece </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Unigram">Unigram </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Reference">Reference </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022_07_07_byte_pair_encoding.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Subword Tokenization</strong></p>
<p>Subword Tokenization이란 띄어쓰기(space)를 기준으로 나뉘는 단어보다 작지만 character(자/모)보다 큰 유닛(subword)으로 문장을 나누는 것으로 다음과 같은 장점이 있다.</p>
<ol>
<li>빈도가 낮은 단어, 사전에 없는 단어들도 (빈도가 높은) 서브 단어들의 조합으로 인코딩할 수 있다.</li>
<li>따라서 적당한 크기의 사전으로 많은 단어를 커버할 수 있다.</li>
<li>띄어쓰기를 안하는 언어(예. 중국어, 일본어) 처리에 용이하다.</li>
<li>접사/어미 등이 실질적 의미를 갖는 어근, 어간에 달라 붙어 어절을 형성하는 교착어인 한국어 처리에 보다 용이하다.
<div class="flash">
    <svg class="octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>형태소 분석을 해서 하나의 형태소를 하나의 subword로 취급하는 것이 아니다. 
</div>
BPE의 다양한 알고리즘을 <a href="https://huggingface.co/docs/transformers/tokenizer_summary#bytepair-encoding-bpe">huggingface의 설명</a>을 참고해 정리해봤다.</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="기본-아이디어">
<a class="anchor" href="#%EA%B8%B0%EB%B3%B8-%EC%95%84%EC%9D%B4%EB%94%94%EC%96%B4" aria-hidden="true"><span class="octicon octicon-link"></span></a>기본 아이디어<a class="anchor-link" href="#%EA%B8%B0%EB%B3%B8-%EC%95%84%EC%9D%B4%EB%94%94%EC%96%B4"> </a>
</h2>
<p><strong>훈련 방식</strong></p>
<ol>
<li>정규화</li>
<li>pre-tokenization &amp; 기본 사전 만들기: 문장을 단어 단위로 나누기. 띄어쓰기 위주의 토큰화(e.g. GPT-2, Roberta 경우)도 가능하지만 보다 복잡한 토큰화를 사용할 수도 있다(GPT, XLM 등). 얻어진 토큰들을 기본 사전으로 삼고, 각 토큰들의 빈도수를 센다.</li>
<li>위에서 구한 기본 사전들의 각 유닛들의 조합들 중 가장 빈도가 높은 조합을 사전에 추가한다.</li>
<li>사전에 정한 크기에 사전이 될 때까지 (2)를 반복한다. </li>
</ol>
<p><strong>예시</strong></p>
<p>(1) 사용하는 corpus가 pre-tokenization 이후 다음과 같은 토큰들과 빈도수를 갖는다고 가정할 때</p>

<pre><code>frquency = [("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)]
vocabulary = ['b, 'g', 'h', 'n', 'p', 's', 'u']
&gt; &gt; [("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)]</code></pre>
<p>(2) 사전에 등록된 아이템 조합 중 'u'+'g'가 가장 빈도가 높으므로 'ug'를 사전에 추가한다.</p>

<pre><code>("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)
&gt;&gt; vocabulary = ['b, 'g', 'h', 'n', 'p', 's', 'u', 'ug'] # update</code></pre>
<p>(3) 정해진 사전 크기까지 '조합 + 사전 등록' 반복.</p>
<blockquote>
<p>note:<code>&lt;/w&gt;</code>와 같은 특별 토큰을 단어 끝에 붙여 단어간 경계를 표기하고 훈련시키기도 한다.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Byte-level-BPE">
<a class="anchor" href="#Byte-level-BPE" aria-hidden="true"><span class="octicon octicon-link"></span></a>Byte-level BPE<a class="anchor-link" href="#Byte-level-BPE"> </a>
</h2>
<p>Unicode가 아닌 Byte로 표현해 사전을 구성하기도 한다. 예를 들어 GPT-2는 바이트 기반으로 기본 사전을 만들기 때문에 256(==2^8)이라는 작은 크기의 기본 사전으로 1) 모든 영문자, 숫자와 일부 특수문자를 커버하고 2) 이들을 결합해 만든 50,000개의 조합과 <end-of-text>이라는 스페셜 토큰을 추가해 총 50,257짜리 사전을 구성한다.&lt;/p&gt;
<p><strong>문제점</strong>
빈도수가 똑같은 서브워드 쌍(pair)들이 여러 있을 때 어떤 쌍을 우선시할지 애매하다. 추가되는 쌍에 따라 같은 단어가 여러가지 방법으로 다르게 인코딩 될 수 있으며, 이는 최종 성능 평가에 영향을 줄 수 있다.</p>

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="WordPiece">
<a class="anchor" href="#WordPiece" aria-hidden="true"><span class="octicon octicon-link"></span></a>WordPiece<a class="anchor-link" href="#WordPiece"> </a>
</h2>
<p>BPE와 기본 아이디어와 훈련 방식이 거의 비슷한 알고리즘이 여러 있다. BERT등이 사용하는 WordPiece도 그 중 하나인데, 사전에 추가하는 기준이 살짝 다르다. BPE는 단순 빈도로 평가했다면, WordPiece는 가능도(likelihood)를 따진다. 또 위 BPE는 바이트들을 결합시켜 새 유닛을 만드는 bottom-up 방식인 반면, WordPiece는 bottom-up은 물론 top-down 방식으로도 구현할 수 있다(한국어, 일본어, 중국어 등은 top-down은 안됨).</p>
<p>WordPiece를 소개한 <a href="https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf">논문</a>을 보면 language model을 만들고 bigram으로 가능도를 측정한다고 되어있다. huggingface <a href="https://huggingface.co/course/chapter6/6?fw=pt">설명</a>에 따르면 이 가능도는 $\frac{C(w_n)}{C(w_{n-1}) \times C(w_n)}$ 로 계산한다. 직관적으로 이해하자면, 각 단어의 빈도가 적을수록(분모) + 둘이 같이 나타나는 빈도가 높을수록(분자) 결합 순위가 높아진다는 것이다. 다만 huggingface측이 제시한 수식이 일반적으로 n-gram language model에서 maximum likelihood 구하는 수식이랑 달라 (분모에서 C(w_n) 추가로 곱하는 것) 조금 의문이 생기는데, huggingface도 구글이 코드를 공개하지 않아 추측해서 만들었다고 한다.
</p>
<div class="flash flash-error">
    <svg class="octicon octicon-alert" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.22 1.754a.25.25 0 00-.44 0L1.698 13.132a.25.25 0 00.22.368h12.164a.25.25 0 00.22-.368L8.22 1.754zm-1.763-.707c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0114.082 15H1.918a1.75 1.75 0 01-1.543-2.575L6.457 1.047zM9 11a1 1 0 11-2 0 1 1 0 012 0zm-.25-5.25a.75.75 0 00-1.5 0v2.5a.75.75 0 001.5 0v-2.5z"></path></svg>
    <strong>Warning: </strong>"Google never open-sourced its implementation of the training algorithm of WordPiece, so what follows is our best guess based on the published literature. It may not be 100% accurate." (huggingface)
</div>
원 논문의 알고리즘은 bottom-up인데, BERT에 사용되었다는 top-down WordPiece 알고리즘은 BPE처럼 단순 빈도를 따지고 pair 구성도 bigram이상으로 하는 등 여러모로 독특하다. <a href="https://www.tensorflow.org/text/guide/subwords_tokenizer#optional_the_algorithm">여기</a> 참고. 같은 character라도 prefix가 있는 경우('h + ug' vs. 'h + '##ug')와 없는 경우 다르게 encoding하는 것도 원 논문에는 없지만 일반적으로 많이 구현하는 방식인듯 하다.

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Unigram">
<a class="anchor" href="#Unigram" aria-hidden="true"><span class="octicon octicon-link"></span></a>Unigram<a class="anchor-link" href="#Unigram"> </a>
</h3>
<p>Unigram은 BPE나 WordPiece와 확연히 다르게 큰 사전으로 시작해서, loss 상 없어져도 큰 상관이 없는 토큰을 없애는 방식으로 사전의 크기를 조정한다. 얼마나 상관이 있는지 계산하는 방식은 이름 그대로 unigramg하게 정한다. 예를 들어 corpus가 다음과 같은 (subword, count)로 이뤄져 있다면 가능한 subword 조합은 다음과 같다.</p>

<pre><code>("dog", 10) ("do", 5) ==&gt; ("d", 15) ("o", 15) ("g", 10) ("do", 15) ("og", 10)</code></pre>
<p>이를 바탕으로 unigram 확률을 구하자면
$$ 
P(["d", "o", "g"]) = P("d") \cdot P("o") \cdot P("g") = \frac{15}{65} \frac{15}{65} \frac{10}{65} = 0.008192990441511151 \\
P(["do", "g"]) = P("do") \cdot P("g") = \frac{15}{65} \frac{10}{65} = 0.03550295857988166 \\
P(["d", "og"]) = P("d") \cdot P("og") = \frac{15}{65} \frac{10}{65} = 0.03550295857988166
$$
이다. 즉 서브워드 토큰의 unigram 확률의 분모가 조합가능한 모든 페어의 빈도를 합한 값이므로 최대한 적은 수의 서브워드로 나누는 것이 확률값이 크다. 이와 같이 unigram상 확률의 negative log likelihood로 훈련 코퍼스에 대한 총 loss를 구하게 된다.
$$
\mathcal{L} = - \log \prod_{t}^{T}{P(w_t)} = - \sum_{t}^{T}{\log P(w_t)} 
$$
인데 모든 subword pair 들 중 편의상 세 조합("o", "og", "do")만을 골라 loss를 비교해 보면,
1) "o"라는 subword를 없앤 후 loss 

$$ \mathcal{L} = - \sum_{t}^{T}{\log P(w_t)} = - 10 \cdot \log P(["do", "g"]) - 5 \cdot \log P(["do"]) = 40.713077800917326 $$

2)  "og"라는 subword를 없앤 후 loss

$$ \mathcal{L} = - \sum_{t}^{T}{\log P(w_t)} = - 10 \cdot \log P(["do", "g"]) - 5 \cdot \log P(["do"]) = 40.713077800917326 $$
</p>
<p>"o" 또는 "og"를 제외한 조합으로 총 loss를 계산했을 때 두 loss간의 차이는 없다 (애초에 "do"로 분할하기 때문에). 그러나</p>
<p>3) "do"라는 subword pair를 사전에서 없애면

$$ \mathcal{L} = - 10 \cdot \log P(["d", "og"]) - 5 \cdot \log P(["d", "o"]) = 48.044763144884456 $$
</p>
<p>오히려 loss가 증가한다. 따라서 세 후보 중 없앴을 때 loss가 최소로 증가하는 pair인 "o" 나 "og"를 사전에서 제거하게 된다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Reference">
<a class="anchor" href="#Reference" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reference<a class="anchor-link" href="#Reference"> </a>
</h2>
<ol>
<li>huggingface: <a href="https://huggingface.co/docs/transformers/tokenizer_summary#bytepair-encoding-bpe">BPE Tokenizer Summary</a>
</li>
<li>huggingface: <a href="https://huggingface.co/course/chapter6/5?fw=pt#bytepair-encoding-tokenization">BPE Tokenization</a>
</li>
<li>google-tensorflow: <a href="https://www.tensorflow.org/text/guide/subwords_tokenizer#optional_the_algorithm">Subword Tokenization</a>
</li>
<li>huggingface: <a href="https://huggingface.co/course/chapter6/5?fw=pt">Byte-Pair Encoding</a>
</li>
<li>huggingface: <a href="https://huggingface.co/course/chapter6/6?fw=pt">WordPiece</a>
</li>
<li>huggingface: <a href="https://huggingface.co/course/chapter6/7?fw=pt">Unigram</a>
</li>
</ol>

</div>
</div>
</div>
&lt;/div&gt;
 

</end-of-text></p>
</div>
</div></div>
</div>


  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="wjlee-ling/nlp_log"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/nlp_log/word%20embedding/bpe/wordpiece/unigram/sentencepiece/subword/2022/07/07/_07_07_byte_pair_encoding.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/nlp_log/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/nlp_log/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/nlp_log/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>언어학 전공한 NLP 개발자</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/wjlee-ling" target="_blank" title="wjlee-ling"><svg class="svg-icon grey"><use xlink:href="/nlp_log/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
